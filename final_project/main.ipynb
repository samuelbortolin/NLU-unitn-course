{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRIbUM44pMbV"
      },
      "source": [
        "# 2-Step Sentiment Analysis for Movie Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgJoKb83GZdn"
      },
      "source": [
        "**[SA2] Sentiment Analysis 2 project**\n",
        "\n",
        "Objective: Classification of Movie Reviews into positive and negative.\n",
        "\n",
        "Implement 2-step classification:\n",
        "* Sentence-level subjectivity detection;\n",
        "* Aggregate into document-level sentiment-polarity.\n",
        "\n",
        "Data Set: [Movie Reviews](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "\n",
        "Evaluation: [F1 score](https://en.wikipedia.org/wiki/F-score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IYEqeWD4rC2"
      },
      "source": [
        "Download English language model from [spaCy](https://spacy.io/models/en#en_core_web_lg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MMaklVsPGNQ",
        "outputId": "29b2bb60-fc82-457f-bb08-ab3159ecaf9b"
      },
      "source": [
        "!pip install spacy==3.1.2\n",
        "!python -m spacy download en_core_web_lg-3.1.0 --direct"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy==3.1.2\n",
            "  Downloading spacy-3.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (21.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (0.8.2)\n",
            "Collecting thinc<8.1.0,>=8.0.8\n",
            "  Downloading thinc-8.0.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (621 kB)\n",
            "\u001b[K     |████████████████████████████████| 621 kB 32.2 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.7\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (4.62.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (3.7.4.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (1.0.5)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 112 kB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 10.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (2.23.0)\n",
            "Collecting catalogue<2.1.0,>=2.0.4\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.2) (0.4.1)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy==3.1.2) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.1.2) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.1.2) (5.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.2) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.2) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3.1.2) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.1.2) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.0 pydantic-1.8.2 spacy-3.1.2 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.8 typer-0.3.2\n",
            "Collecting en-core-web-lg==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.1.0/en_core_web_lg-3.1.0-py3-none-any.whl (777.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 777.1 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.1.0) (3.1.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (8.0.8)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.3.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (21.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (4.62.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (5.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBQUr8oM_Vze"
      },
      "source": [
        "Download and import all the needed packages for executing the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYORhNNIeT0W",
        "outputId": "60ecf34d-cefd-416b-98e4-12dd12c048a7"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "nltk.download(\"vader_lexicon\")\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\", \"ner\"])\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "print(nlp.pipeline)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.datasets import IMDB\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f4ccfdfe9b0>), ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer object at 0x7f4cd00ac280>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo7f4Fy8CXEr"
      },
      "source": [
        "Get movie review data using the [IMDB dataset available in PyTorch](https://pytorch.org/text/stable/datasets.html#imdb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rebZm8BYCWar",
        "outputId": "52fa11ca-9791-4bf3-b092-27effb605633"
      },
      "source": [
        "train_labels = []\n",
        "train_reviews = []\n",
        "for label, review in IMDB(split=\"train\"):\n",
        "  train_labels.append(1 if label == \"pos\" else 0)\n",
        "  train_reviews.append(review)\n",
        "\n",
        "\n",
        "test_labels = []\n",
        "test_reviews = []\n",
        "for label, review in IMDB(split=\"test\"):\n",
        "  test_labels.append(1 if label == \"pos\" else 0)\n",
        "  test_reviews.append(review)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 41.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx6hODH-Aqzz"
      },
      "source": [
        "## Baseline using CountVectorizer and SVC for document-level sentiment-polarity classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GSMM8wOAtkd",
        "outputId": "8e68d79b-b49a-4ec9-e62e-891deadb85d0"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "classifier = SVC()\n",
        "\n",
        "vectors = vectorizer.fit_transform(train_reviews + test_reviews)\n",
        "train_vectors = vectors[:len(test_reviews)]\n",
        "test_vectors = vectors[len(test_reviews):]\n",
        "\n",
        "classifier.fit(train_vectors, train_labels)\n",
        "test_labels_predictions = classifier.predict(test_vectors)\n",
        "\n",
        "print(classification_report(test_labels, test_labels_predictions, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.870     0.851     0.861     12500\n",
            "           1      0.854     0.873     0.864     12500\n",
            "\n",
            "    accuracy                          0.862     25000\n",
            "   macro avg      0.862     0.862     0.862     25000\n",
            "weighted avg      0.862     0.862     0.862     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc8i17nuZBEi"
      },
      "source": [
        "## Baseline using [VADER](https://www.nltk.org/api/nltk.sentiment.html#module-nltk.sentiment.vader) for document-level sentiment-polarity classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GqUK5xsZqKJ"
      },
      "source": [
        "analyzer = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlN7eqZUIeX-",
        "outputId": "b091a671-5965-420d-d29e-04918615a0af"
      },
      "source": [
        "train_scores_predictions = [analyzer.polarity_scores(review) for review in train_reviews]\n",
        "train_scores_predictions = [0 if score[\"neg\"] > score[\"pos\"] else 1 for score in train_scores_predictions]\n",
        "\n",
        "print(classification_report(train_labels, train_scores_predictions, digits=3))\n",
        "\n",
        "\n",
        "test_scores_predictions = [analyzer.polarity_scores(review) for review in test_reviews]\n",
        "test_scores_predictions = [0 if score[\"neg\"] > score[\"pos\"] else 1 for score in test_scores_predictions]\n",
        "\n",
        "print(classification_report(test_labels, test_scores_predictions, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.788     0.523     0.629     12500\n",
            "           1      0.643     0.859     0.736     12500\n",
            "\n",
            "    accuracy                          0.691     25000\n",
            "   macro avg      0.716     0.691     0.682     25000\n",
            "weighted avg      0.716     0.691     0.682     25000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.793     0.523     0.630     12500\n",
            "           1      0.644     0.864     0.738     12500\n",
            "\n",
            "    accuracy                          0.693     25000\n",
            "   macro avg      0.719     0.693     0.684     25000\n",
            "weighted avg      0.719     0.693     0.684     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDVouZx9In8w"
      },
      "source": [
        "## Baseline using [VADER](https://www.nltk.org/api/nltk.sentiment.html#module-nltk.sentiment.vader) for sentence-level sentiment-polarity classification and using objectivity remotion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGCmlZ_ADOWD"
      },
      "source": [
        "train_list_of_sentences = [[sent.text for sent in nlp(review).sents] for review in train_reviews]\n",
        "train_scores_predictions = [[analyzer.polarity_scores(sent) for sent in doc] for doc in train_list_of_sentences]\n",
        "\n",
        "\n",
        "test_list_of_sentences = [[sent.text for sent in nlp(review).sents] for review in test_reviews]\n",
        "test_scores_predictions = [[analyzer.polarity_scores(sent) for sent in doc] for doc in test_list_of_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBRy7ES-IEAc"
      },
      "source": [
        "Simple count of labels and take the popular label among the sentences of a review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oetacLePIDQo",
        "outputId": "593e96ab-9bb4-4846-bc88-95210bfc3043"
      },
      "source": [
        "train_labels_predictions = [[0 if sent_score[\"neg\"] > sent_score[\"pos\"] else 1 for sent_score in doc] for doc in train_scores_predictions]\n",
        "train_labels_predictions = [0 if doc.count(0) > doc.count(1) else 1 for doc in train_labels_predictions]\n",
        "\n",
        "print(classification_report(train_labels, train_labels_predictions, digits=3))\n",
        "\n",
        "\n",
        "test_labels_predictions = [[0 if sent_score[\"neg\"] > sent_score[\"pos\"] else 1 for sent_score in doc] for doc in test_scores_predictions]\n",
        "test_labels_predictions = [0 if doc.count(0) > doc.count(1) else 1 for doc in test_labels_predictions]\n",
        "\n",
        "print(classification_report(test_labels, test_labels_predictions, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.826     0.265     0.401     12500\n",
            "           1      0.562     0.944     0.705     12500\n",
            "\n",
            "    accuracy                          0.605     25000\n",
            "   macro avg      0.694     0.605     0.553     25000\n",
            "weighted avg      0.694     0.605     0.553     25000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.829     0.259     0.395     12500\n",
            "           1      0.561     0.946     0.704     12500\n",
            "\n",
            "    accuracy                          0.603     25000\n",
            "   macro avg      0.695     0.603     0.550     25000\n",
            "weighted avg      0.695     0.603     0.550     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS6RFm9DILWI"
      },
      "source": [
        "Summing the sentence positive and negative contributions to determine the dominant sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_V3Duj1ILGU",
        "outputId": "66ecb69f-1785-44a6-c9bb-46d0999ed006"
      },
      "source": [
        "train_labels_predictions = []\n",
        "for doc in train_scores_predictions:\n",
        "  positive_score = 0.0\n",
        "  negative_score = 0.0\n",
        "  for sent_score in doc:\n",
        "    positive_score += sent_score[\"pos\"]\n",
        "    negative_score += sent_score[\"neg\"]\n",
        "  train_labels_predictions.append(0 if negative_score > positive_score else 1)\n",
        "\n",
        "print(classification_report(train_labels, train_labels_predictions, digits=3))\n",
        "\n",
        "\n",
        "test_labels_predictions = []\n",
        "for doc in test_scores_predictions:\n",
        "  positive_score = 0.0\n",
        "  negative_score = 0.0\n",
        "  for sent_score in doc:\n",
        "    positive_score += sent_score[\"pos\"]\n",
        "    negative_score += sent_score[\"neg\"]\n",
        "  test_labels_predictions.append(0 if negative_score > positive_score else 1)\n",
        "\n",
        "print(classification_report(test_labels, test_labels_predictions, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.801     0.544     0.648     12500\n",
            "           1      0.655     0.865     0.745     12500\n",
            "\n",
            "    accuracy                          0.705     25000\n",
            "   macro avg      0.728     0.705     0.697     25000\n",
            "weighted avg      0.728     0.705     0.697     25000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.808     0.545     0.651     12500\n",
            "           1      0.657     0.870     0.749     12500\n",
            "\n",
            "    accuracy                          0.708     25000\n",
            "   macro avg      0.732     0.708     0.700     25000\n",
            "weighted avg      0.732     0.708     0.700     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWmUYvtlJ6s-"
      },
      "source": [
        "Summing the compound information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i9P6SYSuqaS",
        "outputId": "33866131-ab99-4b7f-edf1-9648d4248590"
      },
      "source": [
        "train_labels_predictions = []\n",
        "for doc in train_scores_predictions:\n",
        "  compound_score = 0.0\n",
        "  for sent_score in doc:\n",
        "    compound_score += sent_score[\"compound\"]\n",
        "  train_labels_predictions.append(0 if compound_score < 0 else 1)\n",
        "\n",
        "print(classification_report(train_labels, train_labels_predictions, digits=3))\n",
        "\n",
        "\n",
        "test_labels_predictions = []\n",
        "for doc in test_scores_predictions:\n",
        "  compound_score = 0.0\n",
        "  for sent_score in doc:\n",
        "    compound_score += sent_score[\"compound\"]\n",
        "  test_labels_predictions.append(0 if compound_score < 0 else 1)\n",
        "\n",
        "print(classification_report(test_labels, test_labels_predictions, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.798     0.544     0.647     12500\n",
            "           1      0.654     0.862     0.744     12500\n",
            "\n",
            "    accuracy                          0.703     25000\n",
            "   macro avg      0.726     0.703     0.695     25000\n",
            "weighted avg      0.726     0.703     0.695     25000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.800     0.545     0.649     12500\n",
            "           1      0.655     0.864     0.745     12500\n",
            "\n",
            "    accuracy                          0.705     25000\n",
            "   macro avg      0.728     0.705     0.697     25000\n",
            "weighted avg      0.728     0.705     0.697     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udgdcRyTE1Dx"
      },
      "source": [
        "Taking into account objectivity at sentence level and use this information on the previous approaches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfE2xXYhFFtx",
        "outputId": "56cd9c0e-dda7-4097-839c-f60f45f384bd"
      },
      "source": [
        "train_labels_predictions = [[0 if sent_score[\"neg\"] > sent_score[\"pos\"] else 1 for sent_score in doc if sent_score[\"neg\"] > sent_score[\"neu\"] or sent_score[\"pos\"] > sent_score[\"neu\"]] for doc in train_scores_predictions]\n",
        "train_labels_predictions = [0 if doc.count(0) > doc.count(1) else 1 for doc in train_labels_predictions]\n",
        "\n",
        "print(classification_report(train_labels, train_labels_predictions, digits=3))\n",
        "\n",
        "\n",
        "test_labels_predictions = [[0 if sent_score[\"neg\"] > sent_score[\"pos\"] else 1 for sent_score in doc if sent_score[\"neg\"] > sent_score[\"neu\"] or sent_score[\"pos\"] > sent_score[\"neu\"]] for doc in test_scores_predictions]\n",
        "test_labels_predictions = [0 if doc.count(0) > doc.count(1) else 1 for doc in test_labels_predictions]\n",
        "\n",
        "print(classification_report(test_labels, test_labels_predictions, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.800     0.194     0.312     12500\n",
            "           1      0.541     0.951     0.690     12500\n",
            "\n",
            "    accuracy                          0.573     25000\n",
            "   macro avg      0.670     0.573     0.501     25000\n",
            "weighted avg      0.670     0.573     0.501     25000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.801     0.188     0.304     12500\n",
            "           1      0.540     0.953     0.689     12500\n",
            "\n",
            "    accuracy                          0.570     25000\n",
            "   macro avg      0.670     0.570     0.497     25000\n",
            "weighted avg      0.670     0.570     0.497     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcPQi2T4N75_",
        "outputId": "06290707-2810-4d22-865d-6d430b55e0e9"
      },
      "source": [
        "train_labels_predictions = []\n",
        "for doc in train_scores_predictions:\n",
        "  positive_score = 0.0\n",
        "  negative_score = 0.0\n",
        "  for sent_score in doc:\n",
        "    if sent_score[\"neg\"] > sent_score[\"neu\"] or sent_score[\"pos\"] > sent_score[\"neu\"]:\n",
        "      positive_score += sent_score[\"pos\"]\n",
        "      negative_score += sent_score[\"neg\"]\n",
        "  train_labels_predictions.append(0 if negative_score > positive_score else 1)\n",
        "\n",
        "print(classification_report(train_labels, train_labels_predictions, digits=3))\n",
        "\n",
        "\n",
        "test_labels_predictions = []\n",
        "for doc in test_scores_predictions:\n",
        "  positive_score = 0.0\n",
        "  negative_score = 0.0\n",
        "  for sent_score in doc:\n",
        "    if sent_score[\"neg\"] > sent_score[\"neu\"] or sent_score[\"pos\"] > sent_score[\"neu\"]:\n",
        "      positive_score += sent_score[\"pos\"]\n",
        "      negative_score += sent_score[\"neg\"]\n",
        "  test_labels_predictions.append(0 if negative_score > positive_score else 1)\n",
        "\n",
        "print(classification_report(test_labels, test_labels_predictions, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.781     0.207     0.327     12500\n",
            "           1      0.543     0.942     0.689     12500\n",
            "\n",
            "    accuracy                          0.574     25000\n",
            "   macro avg      0.662     0.574     0.508     25000\n",
            "weighted avg      0.662     0.574     0.508     25000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.790     0.203     0.323     12500\n",
            "           1      0.543     0.946     0.690     12500\n",
            "\n",
            "    accuracy                          0.575     25000\n",
            "   macro avg      0.666     0.575     0.506     25000\n",
            "weighted avg      0.666     0.575     0.506     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMDqR3JmN7un",
        "outputId": "a8c0dd22-c317-4eac-b284-79e8fe91abd0"
      },
      "source": [
        "train_labels_predictions = []\n",
        "for doc in train_scores_predictions:\n",
        "  compound_score = 0.0\n",
        "  for sent_score in doc:\n",
        "    if sent_score[\"neg\"] > sent_score[\"neu\"] or sent_score[\"pos\"] > sent_score[\"neu\"]:\n",
        "      compound_score += sent_score[\"compound\"]\n",
        "  train_labels_predictions.append(0 if compound_score < 0 else 1)\n",
        "\n",
        "print(classification_report(train_labels, train_labels_predictions, digits=3))\n",
        "\n",
        "\n",
        "test_labels_predictions = []\n",
        "for doc in test_scores_predictions:\n",
        "  compound_score = 0.0\n",
        "  for sent_score in doc:\n",
        "    if sent_score[\"neg\"] > sent_score[\"neu\"] or sent_score[\"pos\"] > sent_score[\"neu\"]:\n",
        "      compound_score += sent_score[\"compound\"]\n",
        "  test_labels_predictions.append(0 if compound_score < 0 else 1)\n",
        "\n",
        "print(classification_report(test_labels, test_labels_predictions, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.787     0.209     0.330     12500\n",
            "           1      0.544     0.944     0.690     12500\n",
            "\n",
            "    accuracy                          0.576     25000\n",
            "   macro avg      0.666     0.576     0.510     25000\n",
            "weighted avg      0.666     0.576     0.510     25000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.789     0.202     0.321     12500\n",
            "           1      0.542     0.946     0.689     12500\n",
            "\n",
            "    accuracy                          0.574     25000\n",
            "   macro avg      0.666     0.574     0.505     25000\n",
            "weighted avg      0.666     0.574     0.505     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H-lORgpnrrR"
      },
      "source": [
        "Taking into account objectivity at sentence level and use it to remove objective sentences and then compute the final document-level sentiment-polarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqSc4PSrkoBk",
        "outputId": "bfeae452-b7f0-46cb-b51c-042bb9bf260d"
      },
      "source": [
        "train_labels_predictions = []\n",
        "for i, doc in enumerate(train_scores_predictions):\n",
        "  subjective_text = \" \".join([train_list_of_sentences[i][j] for j, sent_score in enumerate(doc) if sent_score[\"neg\"] > sent_score[\"neu\"] or sent_score[\"pos\"] > sent_score[\"neu\"]])\n",
        "  train_labels_predictions.append(0 if analyzer.polarity_scores(subjective_text)[\"compound\"] < 0 else 1)\n",
        "\n",
        "print(classification_report(train_labels, train_labels_predictions, digits=3))\n",
        "\n",
        "\n",
        "test_labels_predictions = []\n",
        "for i, doc in enumerate(test_scores_predictions):\n",
        "  subjective_text = \" \".join([test_list_of_sentences[i][j] for j, sent_score in enumerate(doc) if sent_score[\"neg\"] > sent_score[\"neu\"] or sent_score[\"pos\"] > sent_score[\"neu\"]])\n",
        "  test_labels_predictions.append(0 if analyzer.polarity_scores(subjective_text)[\"compound\"] < 0 else 1)\n",
        "\n",
        "print(classification_report(test_labels, test_labels_predictions, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.787     0.208     0.329     12500\n",
            "           1      0.544     0.944     0.690     12500\n",
            "\n",
            "    accuracy                          0.576     25000\n",
            "   macro avg      0.665     0.576     0.509     25000\n",
            "weighted avg      0.665     0.576     0.509     25000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.786     0.202     0.321     12500\n",
            "           1      0.542     0.945     0.689     12500\n",
            "\n",
            "    accuracy                          0.573     25000\n",
            "   macro avg      0.664     0.573     0.505     25000\n",
            "weighted avg      0.664     0.573     0.505     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M49gf6zQgXfT"
      },
      "source": [
        "## Objectivity Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKZsMfodgiiq"
      },
      "source": [
        "Download the [Rotten_IMDB subjectivity dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-x0NjBfgd8C",
        "outputId": "dcbc2326-ef19-4e2e-b36d-7a8940bb4cc1"
      },
      "source": [
        "!wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
        "!mkdir rotten_imdb\n",
        "!tar -xvf rotten_imdb.tar.gz -C rotten_imdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-08-31 16:43:38--  http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 519599 (507K) [application/x-gzip]\n",
            "Saving to: ‘rotten_imdb.tar.gz’\n",
            "\n",
            "rotten_imdb.tar.gz  100%[===================>] 507.42K  2.63MB/s    in 0.2s    \n",
            "\n",
            "2021-08-31 16:43:38 (2.63 MB/s) - ‘rotten_imdb.tar.gz’ saved [519599/519599]\n",
            "\n",
            "quote.tok.gt9.5000\n",
            "plot.tok.gt9.5000\n",
            "subjdata.README.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvNu4X0Ug95Z"
      },
      "source": [
        "Read the files and split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcW_OLuSg5nK"
      },
      "source": [
        "def read_file(path: str):\n",
        "  with open(path, encoding = \"ISO-8859-1\") as file_to_read:\n",
        "    content = np.array(file_to_read.readlines())\n",
        "\n",
        "  return np.array([line.strip().lower() for line in content])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cydjhpEFg7oi"
      },
      "source": [
        "objective_sentences = read_file(\"rotten_imdb/plot.tok.gt9.5000\")\n",
        "subjective_sentences = read_file(\"rotten_imdb/quote.tok.gt9.5000\")\n",
        "objective_labels = np.ones(len(objective_sentences))\n",
        "subjective_labels = np.zeros(len(subjective_sentences))\n",
        "X = np.append(objective_sentences, subjective_sentences)\n",
        "y = np.append(objective_labels, subjective_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIgmRC3tg9D1"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWAK1gznhFny"
      },
      "source": [
        "Define functions for preprocessing a review with a embedding-padding process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYBFLjnJQgnw"
      },
      "source": [
        "embedding_size = 300\n",
        "sequence_length = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NRTEQDWhEvD"
      },
      "source": [
        "def preprocess_review(review: str, embedding_size: int, sequence_length: int) -> list:\n",
        "  embedded_review = []\n",
        "  for word in review.split():\n",
        "    vector = nlp.vocab[word].vector.astype(np.float32)\n",
        "    embedded_review.append(vector.tolist())\n",
        "\n",
        "  zero = list(np.zeros(embedding_size, dtype=np.float32))\n",
        "  if len(embedded_review) <= sequence_length:\n",
        "    zeros = [zero for each in range(sequence_length - len(embedded_review))]\n",
        "    padded_review = zeros + embedded_review\n",
        "  else:\n",
        "    padded_review = embedded_review[: sequence_length]\n",
        "\n",
        "  return padded_review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg2CJhuahqFj"
      },
      "source": [
        "x_train = [preprocess_review(x, embedding_size, sequence_length) for x in X_train]\n",
        "x_test = [preprocess_review(x, embedding_size, sequence_length) for x in X_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ0msdGfhsTb"
      },
      "source": [
        "Define dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "724GvWWkhr1Q"
      },
      "source": [
        "class ObjectivityDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews: list, labels: list) -> None:\n",
        "    super().__init__()\n",
        "    self.reviews = reviews\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, index: int) -> tuple:\n",
        "    return torch.tensor(self.reviews[index]), torch.tensor(self.labels[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttnssPIdhu9z"
      },
      "source": [
        "train_dataset = ObjectivityDataset(x_train, y_train)\n",
        "test_dataset = ObjectivityDataset(x_test, y_test)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSHw5geMyk3S"
      },
      "source": [
        "Define the device that will be used for training and testing the neural networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypGjhmndhOmK",
        "outputId": "3f9e1501-31d3-424b-ef76-f2e57efe00c9"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meAbjw2UhNle"
      },
      "source": [
        "Define the neural network for objectivity classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64KB-4msKS0d"
      },
      "source": [
        "class ObjectivityNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "  This neural network will be used to perform Objectivity classification\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, output_size: int, embedding_size: int, sequence_length: int, hidden_dimension: int, layers_number: int) -> None:\n",
        "    super().__init__()\n",
        "    self.output_size = output_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.sequence_length = sequence_length\n",
        "    self.hidden_dimension = hidden_dimension\n",
        "    self.layers_number = layers_number\n",
        "\n",
        "    # GRU layer\n",
        "    self.gru = nn.GRU(embedding_size, hidden_dimension, layers_number, dropout=0.5, batch_first=True, bidirectional=True)\n",
        "\n",
        "    # Dropout and activation function layers\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.activation = nn.LeakyReLU(0.1)\n",
        "\n",
        "    # Linear layers\n",
        "    self.fc = nn.Linear(2 * hidden_dimension * sequence_length, hidden_dimension)\n",
        "    self.fc_out = nn.Linear(hidden_dimension, output_size)\n",
        "\n",
        "  def forward(self, x: torch.tensor, hidden: torch.tensor) -> torch.tensor:\n",
        "    gru_out, hidden = self.gru(x, hidden)\n",
        "    out = self.dropout(gru_out.contiguous().view(x.size(0), -1))\n",
        "    out = self.activation(out)\n",
        "    out = self.fc(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.activation(out)\n",
        "    out = self.fc_out(out)\n",
        "    return out, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6lqKe_GhS07"
      },
      "source": [
        "objectivity_classifier = ObjectivityNetwork(output_size=2, embedding_size=embedding_size, sequence_length=sequence_length, hidden_dimension=128, layers_number=3)\n",
        "objectivity_classifier = objectivity_classifier.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEMd_uQXha22"
      },
      "source": [
        "Define eval function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RogNGw_thd_C"
      },
      "source": [
        "def evaluate(model: ObjectivityNetwork, loader: DataLoader) -> None:\n",
        "  model.eval()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  h = torch.zeros((3 * 2, batch_size, 128)).to(device)\n",
        "  total_loss = 0.0\n",
        "  true_positive = 0\n",
        "  false_negative = 0\n",
        "  positive_predictions = 0\n",
        "  true_negative = 0\n",
        "  false_positive = 0\n",
        "  negative_predictions = 0\n",
        "  for x, y in loader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    if x.size(0) < batch_size:\n",
        "      h = torch.zeros((3 * 2, x.size(0), 128)).to(device)\n",
        "    h = h.detach().clone().to(device)\n",
        "    outputs, h = model(x, h)\n",
        "    loss = criterion(outputs, y.long())\n",
        "    total_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "\n",
        "    for prediction_index, prediction in enumerate(predicted):\n",
        "      label = y[prediction_index].item()\n",
        "      if label == 1 and prediction == label:\n",
        "        true_positive += 1\n",
        "      if prediction == 0 and label == 1:\n",
        "        false_negative += 1\n",
        "      if prediction == 1:\n",
        "        positive_predictions += 1\n",
        "      if label == 0 and prediction == label:\n",
        "        true_negative += 1\n",
        "      if prediction == 1 and label == 0:\n",
        "        false_positive += 1\n",
        "      if prediction == 0:\n",
        "        negative_predictions += 1\n",
        "\n",
        "  positive_precision = true_positive / positive_predictions if positive_predictions != 0 else 0\n",
        "  positive_recall = true_positive / (true_positive + false_negative)\n",
        "  positive_f1_score = 2 * (positive_precision * positive_recall) / (positive_precision + positive_recall) if positive_precision + positive_recall != 0 else 0\n",
        "  negative_precision = true_negative / negative_predictions if negative_predictions != 0 else 0\n",
        "  negative_recall = true_negative / (true_negative + false_positive)\n",
        "  negative_f1_score = 2 * (negative_precision * negative_recall) / (negative_precision + negative_recall) if negative_precision + negative_recall != 0 else 0\n",
        "  overall_f1_score = (positive_f1_score + negative_f1_score)/2\n",
        "  print(f\"loss: {total_loss/len(loader.dataset)}, accuracy: {(true_positive + true_negative)/len(loader.dataset)}, f1 score: {overall_f1_score} (positive: {positive_f1_score}, negative: {negative_f1_score})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WhqomwdhffW"
      },
      "source": [
        "Define training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX_rpLtFhg2a"
      },
      "source": [
        "def train(model: ObjectivityNetwork, loader: DataLoader, epochs: int = 5, lr: float = 0.001, weight_decay: float = 0.0001) -> None:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  model.train()\n",
        "  for i in range(epochs):\n",
        "    h = torch.zeros((3 * 2, batch_size, 128)).to(device)\n",
        "    total_loss = 0.0\n",
        "    true_positive = 0\n",
        "    false_negative = 0\n",
        "    positive_predictions = 0\n",
        "    true_negative = 0\n",
        "    false_positive = 0\n",
        "    negative_predictions = 0\n",
        "    for x, y in loader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      if x.size(0) < batch_size:\n",
        "        h = torch.zeros((3 * 2, x.size(0), 128)).to(device)\n",
        "      h = h.detach().clone().to(device)\n",
        "      outputs, h = model(x, h)\n",
        "      loss = criterion(outputs, y.long())\n",
        "      total_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      for prediction_index, prediction in enumerate(predicted):\n",
        "        label = y[prediction_index].item()\n",
        "        if label == 1 and prediction == label:\n",
        "          true_positive += 1\n",
        "        if prediction == 0 and label == 1:\n",
        "          false_negative += 1\n",
        "        if prediction == 1:\n",
        "          positive_predictions += 1\n",
        "        if label == 0 and prediction == label:\n",
        "          true_negative += 1\n",
        "        if prediction == 1 and label == 0:\n",
        "          false_positive += 1\n",
        "        if prediction == 0:\n",
        "          negative_predictions += 1\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    positive_precision = true_positive / positive_predictions if positive_predictions != 0 else 0\n",
        "    positive_recall = true_positive / (true_positive + false_negative)\n",
        "    positive_f1_score = 2 * (positive_precision * positive_recall) / (positive_precision + positive_recall) if positive_precision + positive_recall != 0 else 0\n",
        "    negative_precision = true_negative / negative_predictions if negative_predictions != 0 else 0\n",
        "    negative_recall = true_negative / (true_negative + false_positive)\n",
        "    negative_f1_score = 2 * (negative_precision * negative_recall) / (negative_precision + negative_recall) if negative_precision + negative_recall != 0 else 0\n",
        "    overall_f1_score = (positive_f1_score + negative_f1_score)/2\n",
        "    print(f\"loss: {total_loss/len(loader.dataset)}, accuracy: {(true_positive + true_negative)/len(loader.dataset)}, f1 score: {overall_f1_score} (positive: {positive_f1_score}, negative: {negative_f1_score})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxd3DWYMh0O7"
      },
      "source": [
        "Train and evaluate the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x-IdH0nh2SO",
        "outputId": "2b028028-4ae9-443c-c4e5-0e86ab67e721"
      },
      "source": [
        "print(\"train set:\")\n",
        "evaluate(objectivity_classifier, train_loader)\n",
        "\n",
        "print(\"test set:\")\n",
        "evaluate(objectivity_classifier, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train set:\n",
            "loss: 0.005458624057471752, accuracy: 0.494875, f1 score: 0.4888436667535828 (positive: 0.5443680234524749, negative: 0.43331931005469077)\n",
            "test set:\n",
            "loss: 0.0055452798902988435, accuracy: 0.4905, f1 score: 0.48487470297516455 (positive: 0.5387052965142599, negative: 0.4310441094360692)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvfcZRxyh3Zj",
        "outputId": "d29d12f7-7b17-4a10-a89a-25bcc1a91b55"
      },
      "source": [
        "train(objectivity_classifier, train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.0028308588732033966, accuracy: 0.836, f1 score: 0.8359701055517368 (positive: 0.8337557019766851, negative: 0.8381845091267884)\n",
            "loss: 0.0017218348290771247, accuracy: 0.91625, f1 score: 0.9162498115620761 (positive: 0.9161241862794192, negative: 0.9163754368447329)\n",
            "loss: 0.0015444577597081661, accuracy: 0.9185, f1 score: 0.9184970658943722 (positive: 0.9180080482897384, negative: 0.918986083499006)\n",
            "loss: 0.001272466266527772, accuracy: 0.937875, f1 score: 0.9378708985085343 (positive: 0.9373660995589161, negative: 0.9383756974581525)\n",
            "loss: 0.0011273515694774688, accuracy: 0.945375, f1 score: 0.9453734218040142 (positive: 0.9450798039462108, negative: 0.9456670396618176)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmdKQlDzh4b-",
        "outputId": "6ff5ba22-27fd-4b67-9caf-5751bb51d87a"
      },
      "source": [
        "print(\"train set:\")\n",
        "evaluate(objectivity_classifier, train_loader)\n",
        "\n",
        "print(\"test set:\")\n",
        "evaluate(objectivity_classifier, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train set:\n",
            "loss: 0.0008216371382586658, accuracy: 0.96, f1 score: 0.9599909979745442 (positive: 0.9593908629441623, negative: 0.9605911330049262)\n",
            "test set:\n",
            "loss: 0.0014615438170731067, accuracy: 0.937, f1 score: 0.936999432994897 (positive: 0.9368104312938816, negative: 0.9371884346959123)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIfPg5BhxxJ0"
      },
      "source": [
        "## Trial using a CNN for objectivity classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8bDyyzRxxf9"
      },
      "source": [
        "class ObjectivityCNN(nn.Module):\n",
        "  \"\"\"\n",
        "  This neural network will be used to perform Objectivity classification\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, output_size: int, embedding_size: int, filters_number: int) -> None:\n",
        "    super().__init__()\n",
        "    self.output_size = output_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.filters_number = filters_number\n",
        "\n",
        "    # Convolutional layers\n",
        "    self.convs = nn.ModuleList([nn.Conv1d(in_channels=embedding_size, out_channels=filters_number, kernel_size=filter_size) for filter_size in range(1, 9)])\n",
        "\n",
        "    # Dropout and activation function layers\n",
        "    self.dropout = nn.Dropout(p=0.4)\n",
        "    self.activation = nn.LeakyReLU(0.1)\n",
        "\n",
        "    # Classifier set of layers\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.BatchNorm1d(8 * filters_number),\n",
        "      self.activation,\n",
        "      self.dropout,\n",
        "      nn.Linear(8 * filters_number, 128),\n",
        "      nn.BatchNorm1d(128),\n",
        "      self.activation,\n",
        "      nn.Linear(128, output_size)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "    cnn_outputs = [conv(x) for conv in self.convs]\n",
        "    pooled_outputs = [nn.MaxPool1d(kernel_size=cnn_out.shape[2])(cnn_out).squeeze(2) for cnn_out in cnn_outputs]\n",
        "    out = self.classifier(torch.cat(pooled_outputs, dim=1))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP6FpapDy34J"
      },
      "source": [
        "cnn_objectivity_classifier = ObjectivityCNN(output_size=2, embedding_size=embedding_size, filters_number=64)\n",
        "cnn_objectivity_classifier = cnn_objectivity_classifier.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olG8FhiH0o66"
      },
      "source": [
        "def evaluate(model: ObjectivityCNN, loader: DataLoader) -> None:\n",
        "  model.eval()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  total_loss = 0.0\n",
        "  true_positive = 0\n",
        "  false_negative = 0\n",
        "  positive_predictions = 0\n",
        "  true_negative = 0\n",
        "  false_positive = 0\n",
        "  negative_predictions = 0\n",
        "  for x, y in loader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    outputs = model(x.swapaxes(1, 2))\n",
        "    loss = criterion(outputs, y.long())\n",
        "    total_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "\n",
        "    for prediction_index, prediction in enumerate(predicted):\n",
        "      label = y[prediction_index].item()\n",
        "      if label == 1 and prediction == label:\n",
        "        true_positive += 1\n",
        "      if prediction == 0 and label == 1:\n",
        "        false_negative += 1\n",
        "      if prediction == 1:\n",
        "        positive_predictions += 1\n",
        "      if label == 0 and prediction == label:\n",
        "        true_negative += 1\n",
        "      if prediction == 1 and label == 0:\n",
        "        false_positive += 1\n",
        "      if prediction == 0:\n",
        "        negative_predictions += 1\n",
        "\n",
        "  positive_precision = true_positive / positive_predictions if positive_predictions != 0 else 0\n",
        "  positive_recall = true_positive / (true_positive + false_negative)\n",
        "  positive_f1_score = 2 * (positive_precision * positive_recall) / (positive_precision + positive_recall) if positive_precision + positive_recall != 0 else 0\n",
        "  negative_precision = true_negative / negative_predictions if negative_predictions != 0 else 0\n",
        "  negative_recall = true_negative / (true_negative + false_positive)\n",
        "  negative_f1_score = 2 * (negative_precision * negative_recall) / (negative_precision + negative_recall) if negative_precision + negative_recall != 0 else 0\n",
        "  overall_f1_score = (positive_f1_score + negative_f1_score)/2\n",
        "  print(f\"loss: {total_loss/len(loader.dataset)}, accuracy: {(true_positive + true_negative)/len(loader.dataset)}, f1 score: {overall_f1_score} (positive: {positive_f1_score}, negative: {negative_f1_score})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f-EbA4g00P3"
      },
      "source": [
        "def train(model: ObjectivityCNN, loader: DataLoader, epochs: int = 5, lr: float = 0.001, weight_decay: float = 0.0001) -> None:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  model.train()\n",
        "  for i in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    true_positive = 0\n",
        "    false_negative = 0\n",
        "    positive_predictions = 0\n",
        "    true_negative = 0\n",
        "    false_positive = 0\n",
        "    negative_predictions = 0\n",
        "    for x, y in loader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      outputs = model(x.swapaxes(1, 2))\n",
        "      loss = criterion(outputs, y.long())\n",
        "      total_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      for prediction_index, prediction in enumerate(predicted):\n",
        "        label = y[prediction_index].item()\n",
        "        if label == 1 and prediction == label:\n",
        "          true_positive += 1\n",
        "        if prediction == 0 and label == 1:\n",
        "          false_negative += 1\n",
        "        if prediction == 1:\n",
        "          positive_predictions += 1\n",
        "        if label == 0 and prediction == label:\n",
        "          true_negative += 1\n",
        "        if prediction == 1 and label == 0:\n",
        "          false_positive += 1\n",
        "        if prediction == 0:\n",
        "          negative_predictions += 1\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    positive_precision = true_positive / positive_predictions if positive_predictions != 0 else 0\n",
        "    positive_recall = true_positive / (true_positive + false_negative)\n",
        "    positive_f1_score = 2 * (positive_precision * positive_recall) / (positive_precision + positive_recall) if positive_precision + positive_recall != 0 else 0\n",
        "    negative_precision = true_negative / negative_predictions if negative_predictions != 0 else 0\n",
        "    negative_recall = true_negative / (true_negative + false_positive)\n",
        "    negative_f1_score = 2 * (negative_precision * negative_recall) / (negative_precision + negative_recall) if negative_precision + negative_recall != 0 else 0\n",
        "    overall_f1_score = (positive_f1_score + negative_f1_score)/2\n",
        "    print(f\"loss: {total_loss/len(loader.dataset)}, accuracy: {(true_positive + true_negative)/len(loader.dataset)}, f1 score: {overall_f1_score} (positive: {positive_f1_score}, negative: {negative_f1_score})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LCz_R09y8qL",
        "outputId": "88542ddd-afec-452e-fbb4-66fe3ec82c7b"
      },
      "source": [
        "print(\"train set:\")\n",
        "evaluate(cnn_objectivity_classifier, train_loader)\n",
        "\n",
        "print(\"test set:\")\n",
        "evaluate(cnn_objectivity_classifier, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train set:\n",
            "loss: 0.005462134793400765, accuracy: 0.492125, f1 score: 0.4796848417918723 (positive: 0.5601385731298041, negative: 0.39923111045394055)\n",
            "test set:\n",
            "loss: 0.005549371659755707, accuracy: 0.506, f1 score: 0.4903932946486138 (positive: 0.5795744680851064, negative: 0.40121212121212124)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sacYWUe4y9-Y",
        "outputId": "a1b9f4f6-1b04-4bd9-e0b1-bfbf325771cc"
      },
      "source": [
        "train(cnn_objectivity_classifier, train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.002072600895538926, accuracy: 0.891125, f1 score: 0.8911178120743127 (positive: 0.8902331442974165, negative: 0.892002479851209)\n",
            "loss: 0.0006555817448534072, accuracy: 0.973875, f1 score: 0.9738746077109064 (positive: 0.9737733718157863, negative: 0.9739758436060266)\n",
            "loss: 0.00024829809193033724, accuracy: 0.990625, f1 score: 0.9906249225091251 (positive: 0.9905979691613389, negative: 0.9906518758569114)\n",
            "loss: 0.00019522384798619896, accuracy: 0.991375, f1 score: 0.9913749157706618 (positive: 0.9913479623824452, negative: 0.9914018691588785)\n",
            "loss: 0.00020271150389453396, accuracy: 0.991625, f1 score: 0.9916249307748185 (positive: 0.9916008524507961, negative: 0.9916490090988409)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfJMX591y_EK",
        "outputId": "358ae532-1b06-4d11-e931-596d6e8db935"
      },
      "source": [
        "print(\"train set:\")\n",
        "evaluate(cnn_objectivity_classifier, train_loader)\n",
        "\n",
        "print(\"test set:\")\n",
        "evaluate(cnn_objectivity_classifier, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train set:\n",
            "loss: 3.8168397979461585e-05, accuracy: 0.999125, f1 score: 0.9991249914549947 (positive: 0.9991222570532915, negative: 0.9991277258566977)\n",
            "test set:\n",
            "loss: 0.001789900705218315, accuracy: 0.933, f1 score: 0.9329957117255503 (positive: 0.9335317460317459, negative: 0.9324596774193548)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv-jz8cjiP8R"
      },
      "source": [
        "## Sentence-level objectivity-remotion and document-level sentiment-polarity classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7E_6HhvtTBj"
      },
      "source": [
        "Remove objective sentences from sentiment dataset using the ObjectivityNetwork trained on Rotten_IMDB dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak2As3HjiW4N"
      },
      "source": [
        "def objectivity_remotion(model: ObjectivityNetwork, review: str, embedding_size: int, sequence_length: int) -> str:\n",
        "  model.eval()\n",
        "  review = review.strip().lower()\n",
        "\n",
        "  # Remove HTML tags and also the \" that can occour in the reviews\n",
        "  review = re.sub(r'<.*?>', \"\", review)\n",
        "  review = re.sub(r'\"', \"\", review)\n",
        "\n",
        "  # Pass inside the ObjectivityNetwork all the sentences and remove the objective ones\n",
        "  sentences = []\n",
        "  padded_sentences = []\n",
        "  for sentence in nlp(review).sents:\n",
        "    sentences.append(sentence.text)\n",
        "    padded_sentences.append(preprocess_review(sentence.text, embedding_size, sequence_length))\n",
        "  h = torch.zeros((3 * 2, len(padded_sentences), 128)).to(device)\n",
        "  outputs, h = model(torch.tensor(padded_sentences).to(device), h)\n",
        "  _, objectivity = outputs.max(1)\n",
        "  if objectivity.all() == 1:  # Maintain all the sententences if all are classified as objectives\n",
        "    subjective_text = \" \".join([sentences[i] for i, item in enumerate(objectivity)])\n",
        "  else:\n",
        "    subjective_text = \" \".join([sentences[i] for i, item in enumerate(objectivity) if item == 0])\n",
        "\n",
        "  return subjective_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0jgCIMXyXfF"
      },
      "source": [
        "train_reviews = [objectivity_remotion(objectivity_classifier, review, embedding_size, sequence_length) for review in train_reviews]\n",
        "test_reviews = [objectivity_remotion(objectivity_classifier, review, embedding_size, sequence_length) for review in test_reviews]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFNhIjnc2bY_"
      },
      "source": [
        "Test again CountVectorizer + SVC on preprocessed data without objective sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg3synWgJgAr",
        "outputId": "b03e96c4-8b56-44a4-ed0c-5db0a145f188"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "classifier = SVC()\n",
        "\n",
        "vectors = vectorizer.fit_transform(train_reviews + test_reviews)\n",
        "train_vectors = vectors[:len(test_reviews)]\n",
        "test_vectors = vectors[len(test_reviews):]\n",
        "\n",
        "classifier.fit(train_vectors, train_labels)\n",
        "test_labels_predictions = classifier.predict(test_vectors)\n",
        "\n",
        "print(classification_report(test_labels, test_labels_predictions, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.877     0.837     0.857     12500\n",
            "           1      0.844     0.883     0.863     12500\n",
            "\n",
            "    accuracy                          0.860     25000\n",
            "   macro avg      0.861     0.860     0.860     25000\n",
            "weighted avg      0.861     0.860     0.860     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uAHSI6JZpzD"
      },
      "source": [
        "Define dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AriK3jUdxIXu"
      },
      "source": [
        "sequence_length = 384"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEFePn9ZO3dP"
      },
      "source": [
        "class SentimentDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews: list, labels: list, embedding_size: int, sequence_length: int) -> None:\n",
        "    super().__init__()\n",
        "    self.reviews = reviews\n",
        "    self.labels = labels\n",
        "    self.embedding_size = embedding_size\n",
        "    self.sequence_length = sequence_length\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, index: int) -> tuple:\n",
        "    return torch.tensor(preprocess_review(self.reviews[index], self.embedding_size, self.sequence_length)), torch.tensor(self.labels[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp4oJMFaOtfg"
      },
      "source": [
        "train_dataset = SentimentDataset(train_reviews, train_labels, embedding_size, sequence_length)\n",
        "test_dataset = SentimentDataset(test_reviews, test_labels, embedding_size, sequence_length)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDZEYJDhZgJh"
      },
      "source": [
        "Define the network for sentiment-polarity classification (using the same structure as for objectivity detection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-rGgFqJAXN8"
      },
      "source": [
        "class SentimentNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "  This neural network will be used to perform sentiment-polarity classification\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, output_size: int, embedding_size: int, sequence_length: int, hidden_dimension: int, layers_number: int) -> None:\n",
        "    super().__init__()\n",
        "    self.output_size = output_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.sequence_length = sequence_length\n",
        "    self.hidden_dimension = hidden_dimension\n",
        "    self.layers_number = layers_number\n",
        "\n",
        "    # GRU layer\n",
        "    self.gru = nn.GRU(embedding_size, hidden_dimension, layers_number, dropout=0.5, batch_first=True, bidirectional=True)\n",
        "\n",
        "    # Dropout and activation function layers\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.activation = nn.LeakyReLU(0.1)\n",
        "\n",
        "    # Linear layers\n",
        "    self.fc = nn.Linear(2 * hidden_dimension * sequence_length, hidden_dimension)\n",
        "    self.fc_out = nn.Linear(hidden_dimension, output_size)\n",
        "\n",
        "  def forward(self, x: torch.tensor, hidden: torch.tensor) -> torch.tensor:\n",
        "    gru_out, hidden = self.gru(x, hidden)\n",
        "    out = self.dropout(gru_out.contiguous().view(x.size(0), -1))\n",
        "    out = self.activation(out)\n",
        "    out = self.fc(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.activation(out)\n",
        "    out = self.fc_out(out)\n",
        "    return out, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIQuYuS4tSs_"
      },
      "source": [
        "sentiment_classifier = SentimentNetwork(output_size=2, embedding_size=embedding_size, sequence_length=sequence_length, hidden_dimension=128, layers_number=3)\n",
        "sentiment_classifier = sentiment_classifier.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnM_lo3M4yW1"
      },
      "source": [
        "Define code for training and evalute the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3PaNY7JG8nR"
      },
      "source": [
        "epochs = 10\n",
        "f1_score_max = 0.0\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(sentiment_classifier.parameters(), lr=0.001, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ9TwVXiPofa",
        "outputId": "bbf46168-e633-4e17-e48b-a59a8b83bdeb"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch + 1}\")\n",
        "  # Training phase\n",
        "  sentiment_classifier.train()\n",
        "  h = torch.zeros((3 * 2, batch_size, 128)).to(device)\n",
        "  total_loss = 0.0\n",
        "  true_positive = 0\n",
        "  false_negative = 0\n",
        "  positive_predictions = 0\n",
        "  true_negative = 0\n",
        "  false_positive = 0\n",
        "  negative_predictions = 0\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    if inputs.size(0) < batch_size:\n",
        "      h = torch.zeros((3 * 2, inputs.size(0), 128)).to(device)\n",
        "\n",
        "    h = h.detach().clone().to(device)\n",
        "    outputs, h = sentiment_classifier(inputs, h)\n",
        "    loss = criterion(outputs, labels)\n",
        "    total_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    for prediction_index, prediction in enumerate(predicted):\n",
        "      label = labels[prediction_index].item()\n",
        "      if label == 1 and prediction == label:\n",
        "        true_positive += 1\n",
        "      if prediction == 0 and label == 1:\n",
        "        false_negative += 1\n",
        "      if prediction == 1:\n",
        "        positive_predictions += 1\n",
        "      if label == 0 and prediction == label:\n",
        "        true_negative += 1\n",
        "      if prediction == 1 and label == 0:\n",
        "        false_positive += 1\n",
        "      if prediction == 0:\n",
        "        negative_predictions += 1\n",
        "\n",
        "  positive_precision = true_positive / positive_predictions if positive_predictions != 0 else 0\n",
        "  positive_recall = true_positive / (true_positive + false_negative)\n",
        "  positive_f1_score = 2 * (positive_precision * positive_recall) / (positive_precision + positive_recall) if positive_precision + positive_recall != 0 else 0\n",
        "  negative_precision = true_negative / negative_predictions if negative_predictions != 0 else 0\n",
        "  negative_recall = true_negative / (true_negative + false_positive)\n",
        "  negative_f1_score = 2 * (negative_precision * negative_recall) / (negative_precision + negative_recall) if negative_precision + negative_recall != 0 else 0\n",
        "  overall_f1_score = (positive_f1_score + negative_f1_score)/2\n",
        "  print(f\"train loss: {total_loss/len(train_loader.dataset)}, train accuracy: {(true_positive + true_negative)/len(train_loader.dataset)}, train f1 score: {overall_f1_score} (positive: {positive_f1_score}, negative: {negative_f1_score})\")\n",
        "\n",
        "  # Evaluation phase\n",
        "  sentiment_classifier.eval()\n",
        "  h = torch.zeros((3 * 2, batch_size, 128)).to(device)\n",
        "  total_loss = 0.0\n",
        "  true_positive = 0\n",
        "  false_negative = 0\n",
        "  positive_predictions = 0\n",
        "  true_negative = 0\n",
        "  false_positive = 0\n",
        "  negative_predictions = 0\n",
        "  for inputs, labels in test_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    if inputs.size(0) < batch_size:\n",
        "      h = torch.zeros((3 * 2, inputs.size(0), 128)).to(device)\n",
        "\n",
        "    h = h.detach().clone().to(device)\n",
        "    outputs, h = sentiment_classifier(inputs, h)\n",
        "    loss = criterion(outputs, labels)\n",
        "    total_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "\n",
        "    for prediction_index, prediction in enumerate(predicted):\n",
        "      label = labels[prediction_index].item()\n",
        "      if label == 1 and prediction == label:\n",
        "        true_positive += 1\n",
        "      if prediction == 0 and label == 1:\n",
        "        false_negative += 1\n",
        "      if prediction == 1:\n",
        "        positive_predictions += 1\n",
        "      if label == 0 and prediction == label:\n",
        "        true_negative += 1\n",
        "      if prediction == 1 and label == 0:\n",
        "        false_positive += 1\n",
        "      if prediction == 0:\n",
        "        negative_predictions += 1\n",
        "\n",
        "  positive_precision = true_positive / positive_predictions if positive_predictions != 0 else 0\n",
        "  positive_recall = true_positive / (true_positive + false_negative)\n",
        "  positive_f1_score = 2 * (positive_precision * positive_recall) / (positive_precision + positive_recall) if positive_precision + positive_recall != 0 else 0\n",
        "  negative_precision = true_negative / negative_predictions if negative_predictions != 0 else 0\n",
        "  negative_recall = true_negative / (true_negative + false_positive)\n",
        "  negative_f1_score = 2 * (negative_precision * negative_recall) / (negative_precision + negative_recall) if negative_precision + negative_recall != 0 else 0\n",
        "  overall_f1_score = (positive_f1_score + negative_f1_score)/2\n",
        "  print(f\"test loss: {total_loss/len(test_loader.dataset)}, test accuracy: {(true_positive + true_negative)/len(test_loader.dataset)}, test f1 score: {overall_f1_score} (positive: {positive_f1_score}, negative: {negative_f1_score})\")\n",
        "\n",
        "  if overall_f1_score >= f1_score_max:\n",
        "    print(f\"Increase in f1 score from {f1_score_max} to {overall_f1_score}\")\n",
        "    torch.save(sentiment_classifier.state_dict(), \"SentimentNetwork.pth\")\n",
        "    f1_score_max = overall_f1_score\n",
        "  print(10 * \"================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "train loss: 0.003938392232060432, train accuracy: 0.75024, train f1 score: 0.750186457563375 (positive: 0.7465291873021028, negative: 0.753843727824647)\n",
            "test loss: 0.0026351734215021134, test accuracy: 0.856, test f1 score: 0.8558213568940459 (positive: 0.8507462686567164, negative: 0.8608964451313754)\n",
            "Increase in f1 score from 0.0 to 0.8558213568940459\n",
            "================================================================================================================================================================\n",
            "Epoch 2\n",
            "train loss: 0.0025699855095148087, train accuracy: 0.86016, train f1 score: 0.8601571932989324 (positive: 0.8595306975249116, negative: 0.8607836890729531)\n",
            "test loss: 0.0024470092803239823, test accuracy: 0.86604, test f1 score: 0.8656660656961948 (positive: 0.858578607322326, negative: 0.8727535240700635)\n",
            "Increase in f1 score from 0.8558213568940459 to 0.8656660656961948\n",
            "================================================================================================================================================================\n",
            "Epoch 3\n",
            "train loss: 0.0023251209831237794, train accuracy: 0.87448, train f1 score: 0.8744798425475144 (positive: 0.8743392599711676, negative: 0.8746204251238613)\n",
            "test loss: 0.002685638816356659, test accuracy: 0.86472, test f1 score: 0.863973507502845 (positive: 0.8538966649386557, negative: 0.8740503500670341)\n",
            "================================================================================================================================================================\n",
            "Epoch 4\n",
            "train loss: 0.0020462197527289392, train accuracy: 0.8912, train f1 score: 0.8912 (positive: 0.8912, negative: 0.8912)\n",
            "test loss: 0.0022773512142896654, test accuracy: 0.88584, test f1 score: 0.8858362123195178 (positive: 0.8864937957365574, negative: 0.8851786289024783)\n",
            "Increase in f1 score from 0.8656660656961948 to 0.8858362123195178\n",
            "================================================================================================================================================================\n",
            "Epoch 5\n",
            "train loss: 0.0017819966793060304, train accuracy: 0.90536, train f1 score: 0.9053598037540891 (positive: 0.9054960856366833, negative: 0.9052235218714948)\n",
            "test loss: 0.0024790741658210756, test accuracy: 0.87076, test f1 score: 0.8701730239911138 (positive: 0.8789025898579513, negative: 0.8614434581242763)\n",
            "================================================================================================================================================================\n",
            "Epoch 6\n",
            "train loss: 0.0014873124173283576, train accuracy: 0.92148, train f1 score: 0.9214798631865135 (positive: 0.9213762166059198, negative: 0.9215835097671073)\n",
            "test loss: 0.0026431910932064057, test accuracy: 0.86936, test f1 score: 0.8687602659127575 (positive: 0.8776320719370551, negative: 0.8598884598884599)\n",
            "================================================================================================================================================================\n",
            "Epoch 7\n",
            "train loss: 0.0012401974087953567, train accuracy: 0.93604, train f1 score: 0.9360399630566827 (positive: 0.9360885726847595, negative: 0.9359913534286057)\n",
            "test loss: 0.002622128041982651, test accuracy: 0.8774, test f1 score: 0.8772468651696917 (positive: 0.8729112244474853, negative: 0.8815825058918982)\n",
            "================================================================================================================================================================\n",
            "Epoch 8\n",
            "train loss: 0.0010164019694924354, train accuracy: 0.9464, train f1 score: 0.9463999008612567 (positive: 0.946327004726428, negative: 0.9464727969960853)\n",
            "test loss: 0.0032231508976221083, test accuracy: 0.87044, test f1 score: 0.8699368063137389 (positive: 0.8780267369610243, negative: 0.8618468756664535)\n",
            "================================================================================================================================================================\n",
            "Epoch 9\n",
            "train loss: 0.0007774481169879437, train accuracy: 0.9606, train f1 score: 0.9605999893462371 (positive: 0.960620477351777, negative: 0.9605795013406973)\n",
            "test loss: 0.0033365495359897615, test accuracy: 0.8784, test f1 score: 0.8783713041904491 (positive: 0.8802395209580838, negative: 0.8765030874228145)\n",
            "================================================================================================================================================================\n",
            "Epoch 10\n",
            "train loss: 0.0006275947224348784, train accuracy: 0.96868, train f1 score: 0.9686799819095575 (positive: 0.9687037851233062, negative: 0.9686561786958089)\n",
            "test loss: 0.003966312516927719, test accuracy: 0.8824, test f1 score: 0.8823939032999469 (positive: 0.8832406671961873, negative: 0.8815471394037065)\n",
            "================================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsOF7lh_TjRk"
      },
      "source": [
        "Use the stored SentimentNetwork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWxWqiZeUak8",
        "outputId": "4bea8604-0bee-4a4c-ec77-32a0370e061e"
      },
      "source": [
        "sentiment_classifier.load_state_dict(torch.load(\"SentimentNetwork.pth\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6vfvdYVUjaE",
        "outputId": "24c4a128-c164-4e86-b530-94accacb4e30"
      },
      "source": [
        "sentiment_classifier.eval()\n",
        "h = torch.zeros((3 * 2, batch_size, 128)).to(device)\n",
        "total_loss = 0.0\n",
        "true_positive = 0\n",
        "false_negative = 0\n",
        "positive_predictions = 0\n",
        "true_negative = 0\n",
        "false_positive = 0\n",
        "negative_predictions = 0\n",
        "for inputs, labels in test_loader:\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  if inputs.size(0) < batch_size:\n",
        "    h = torch.zeros((3 * 2, inputs.size(0), 128)).to(device)\n",
        "\n",
        "  h = h.detach().clone().to(device)\n",
        "  outputs, h = sentiment_classifier(inputs, h)\n",
        "  loss = criterion(outputs, labels)\n",
        "  total_loss += loss.item()\n",
        "  _, predicted = outputs.max(1)\n",
        "\n",
        "  for prediction_index, prediction in enumerate(predicted):\n",
        "    label = labels[prediction_index].item()\n",
        "    if label == 1 and prediction == label:\n",
        "      true_positive += 1\n",
        "    if prediction == 0 and label == 1:\n",
        "      false_negative += 1\n",
        "    if prediction == 1:\n",
        "      positive_predictions += 1\n",
        "    if label == 0 and prediction == label:\n",
        "      true_negative += 1\n",
        "    if prediction == 1 and label == 0:\n",
        "      false_positive += 1\n",
        "    if prediction == 0:\n",
        "      negative_predictions += 1\n",
        "\n",
        "positive_precision = true_positive / positive_predictions if positive_predictions != 0 else 0\n",
        "positive_recall = true_positive / (true_positive + false_negative)\n",
        "positive_f1_score = 2 * (positive_precision * positive_recall) / (positive_precision + positive_recall) if positive_precision + positive_recall != 0 else 0\n",
        "negative_precision = true_negative / negative_predictions if negative_predictions != 0 else 0\n",
        "negative_recall = true_negative / (true_negative + false_positive)\n",
        "negative_f1_score = 2 * (negative_precision * negative_recall) / (negative_precision + negative_recall) if negative_precision + negative_recall != 0 else 0\n",
        "overall_f1_score = (positive_f1_score + negative_f1_score)/2\n",
        "print(f\"test loss: {total_loss/len(test_loader.dataset)}, test accuracy: {(true_positive + true_negative)/len(test_loader.dataset)}, test f1 score: {overall_f1_score} (positive: {positive_f1_score}, negative: {negative_f1_score})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss: 0.0022735300797224046, test accuracy: 0.88592, test f1 score: 0.885917102111861 (positive: 0.8864920799172171, negative: 0.8853421243065048)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh0eiNgSnJhf"
      },
      "source": [
        "## Trial using a CNN for document-level sentiment-polarity classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV7z6aalKdsq"
      },
      "source": [
        "class SentimentCNN(nn.Module):\n",
        "  \"\"\"\n",
        "  This neural network will be used to perform sentiment-polarity classification\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, output_size: int, embedding_size: int, filters_number: int) -> None:\n",
        "    super().__init__()\n",
        "    self.output_size = output_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.filters_number = filters_number\n",
        "\n",
        "    # Convolutional layers\n",
        "    self.convs = nn.ModuleList([nn.Conv1d(in_channels=embedding_size, out_channels=filters_number, kernel_size=filter_size) for filter_size in range(1, 9)])\n",
        "\n",
        "    # Dropout and activation function layers\n",
        "    self.dropout = nn.Dropout(p=0.4)\n",
        "    self.activation = nn.LeakyReLU(0.1)\n",
        "\n",
        "    # Classifier set of layers\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.BatchNorm1d(8 * filters_number),\n",
        "      self.activation,\n",
        "      self.dropout,\n",
        "      nn.Linear(8 * filters_number, 128),\n",
        "      nn.BatchNorm1d(128),\n",
        "      self.activation,\n",
        "      nn.Linear(128, output_size)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "    cnn_outputs = [conv(x) for conv in self.convs]\n",
        "    pooled_outputs = [nn.MaxPool1d(kernel_size=cnn_out.shape[2])(cnn_out).squeeze(2) for cnn_out in cnn_outputs]\n",
        "    out = self.classifier(torch.cat(pooled_outputs, dim=1))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5YbDyqkKfMc"
      },
      "source": [
        "cnn_sentiment_classifier = SentimentCNN(output_size=2, embedding_size=embedding_size, filters_number=64)\n",
        "cnn_sentiment_classifier = cnn_sentiment_classifier.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_hjL6b_KhIv"
      },
      "source": [
        "epochs = 10\n",
        "f1_score_max = 0.0\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(cnn_sentiment_classifier.parameters(), lr=0.001, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1sdx5apKiaS",
        "outputId": "743db332-74a4-4a0a-da94-e6199f83b4d8"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch + 1}\")\n",
        "  # Training phase\n",
        "  cnn_sentiment_classifier.train()\n",
        "  total_loss = 0.0\n",
        "  true_positive = 0\n",
        "  false_negative = 0\n",
        "  positive_predictions = 0\n",
        "  true_negative = 0\n",
        "  false_positive = 0\n",
        "  negative_predictions = 0\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = cnn_sentiment_classifier(inputs.swapaxes(1, 2))\n",
        "    loss = criterion(outputs, labels)\n",
        "    total_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    for prediction_index, prediction in enumerate(predicted):\n",
        "      label = labels[prediction_index].item()\n",
        "      if label == 1 and prediction == label:\n",
        "        true_positive += 1\n",
        "      if prediction == 0 and label == 1:\n",
        "        false_negative += 1\n",
        "      if prediction == 1:\n",
        "        positive_predictions += 1\n",
        "      if label == 0 and prediction == label:\n",
        "        true_negative += 1\n",
        "      if prediction == 1 and label == 0:\n",
        "        false_positive += 1\n",
        "      if prediction == 0:\n",
        "        negative_predictions += 1\n",
        "\n",
        "  positive_precision = true_positive / positive_predictions if positive_predictions != 0 else 0\n",
        "  positive_recall = true_positive / (true_positive + false_negative)\n",
        "  positive_f1_score = 2 * (positive_precision * positive_recall) / (positive_precision + positive_recall) if positive_precision + positive_recall != 0 else 0\n",
        "  negative_precision = true_negative / negative_predictions if negative_predictions != 0 else 0\n",
        "  negative_recall = true_negative / (true_negative + false_positive)\n",
        "  negative_f1_score = 2 * (negative_precision * negative_recall) / (negative_precision + negative_recall) if negative_precision + negative_recall != 0 else 0\n",
        "  overall_f1_score = (positive_f1_score + negative_f1_score)/2\n",
        "  print(f\"train loss: {total_loss/len(train_loader.dataset)}, train accuracy: {(true_positive + true_negative)/len(train_loader.dataset)}, train f1 score: {overall_f1_score} (positive: {positive_f1_score}, negative: {negative_f1_score})\")\n",
        "\n",
        "  # Evaluation phase\n",
        "  cnn_sentiment_classifier.eval()\n",
        "  total_loss = 0.0\n",
        "  true_positive = 0\n",
        "  false_negative = 0\n",
        "  positive_predictions = 0\n",
        "  true_negative = 0\n",
        "  false_positive = 0\n",
        "  negative_predictions = 0\n",
        "  for inputs, labels in test_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = cnn_sentiment_classifier(inputs.swapaxes(1, 2))\n",
        "    loss = criterion(outputs, labels)\n",
        "    total_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "        \n",
        "    for prediction_index, prediction in enumerate(predicted):\n",
        "      label = labels[prediction_index].item()\n",
        "      if label == 1 and prediction == label:\n",
        "        true_positive += 1\n",
        "      if prediction == 0 and label == 1:\n",
        "        false_negative += 1\n",
        "      if prediction == 1:\n",
        "        positive_predictions += 1\n",
        "      if label == 0 and prediction == label:\n",
        "        true_negative += 1\n",
        "      if prediction == 1 and label == 0:\n",
        "        false_positive += 1\n",
        "      if prediction == 0:\n",
        "        negative_predictions += 1\n",
        "\n",
        "  positive_precision = true_positive / positive_predictions if positive_predictions != 0 else 0\n",
        "  positive_recall = true_positive / (true_positive + false_negative)\n",
        "  positive_f1_score = 2 * (positive_precision * positive_recall) / (positive_precision + positive_recall) if positive_precision + positive_recall != 0 else 0\n",
        "  negative_precision = true_negative / negative_predictions if negative_predictions != 0 else 0\n",
        "  negative_recall = true_negative / (true_negative + false_positive)\n",
        "  negative_f1_score = 2 * (negative_precision * negative_recall) / (negative_precision + negative_recall) if negative_precision + negative_recall != 0 else 0\n",
        "  overall_f1_score = (positive_f1_score + negative_f1_score)/2\n",
        "  print(f\"test loss: {total_loss/len(test_loader.dataset)}, test accuracy: {(true_positive + true_negative)/len(test_loader.dataset)}, test f1 score: {overall_f1_score} (positive: {positive_f1_score}, negative: {negative_f1_score})\")\n",
        "\n",
        "  if overall_f1_score >= f1_score_max:\n",
        "    print(f\"Increase in f1 score from {f1_score_max} to {overall_f1_score}\")\n",
        "    torch.save(cnn_sentiment_classifier.state_dict(), \"SentimentCNN.pth\")\n",
        "    f1_score_max = overall_f1_score\n",
        "  print(10 * \"================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "train loss: 0.0030546901553869248, train accuracy: 0.82068, train f1 score: 0.8206458437944922 (positive: 0.8231209311501282, negative: 0.8181707564388563)\n",
            "test loss: 0.002373743687868118, test accuracy: 0.87124, test f1 score: 0.87123465963778 (positive: 0.8720639084297127, negative: 0.8704054108458472)\n",
            "Increase in f1 score from 0.0 to 0.87123465963778\n",
            "================================================================================================================================================================\n",
            "Epoch 2\n",
            "train loss: 0.0020627069264650346, train accuracy: 0.89036, train f1 score: 0.8903546273767413 (positive: 0.8911221449851041, negative: 0.8895871097683786)\n",
            "test loss: 0.002300197804570198, test accuracy: 0.8754, test f1 score: 0.8753993522759929 (positive: 0.8756834417528037, negative: 0.8751152627991822)\n",
            "Increase in f1 score from 0.87123465963778 to 0.8753993522759929\n",
            "================================================================================================================================================================\n",
            "Epoch 3\n",
            "train loss: 0.0015444465312361717, train accuracy: 0.92188, train f1 score: 0.9218791389206209 (positive: 0.9221385001794044, negative: 0.9216197776618373)\n",
            "test loss: 0.0023242112112045288, test accuracy: 0.87888, test f1 score: 0.8788782876296792 (positive: 0.8793337052681917, negative: 0.8784228699911668)\n",
            "Increase in f1 score from 0.8753993522759929 to 0.8788782876296792\n",
            "================================================================================================================================================================\n",
            "Epoch 4\n",
            "train loss: 0.0011438279397785664, train accuracy: 0.943, train f1 score: 0.9429991061347827 (positive: 0.9432248296744891, negative: 0.9427733825950764)\n",
            "test loss: 0.002522771019935608, test accuracy: 0.87668, test f1 score: 0.8766796998877177 (positive: 0.8764873202195425, negative: 0.8768720795558929)\n",
            "================================================================================================================================================================\n",
            "Epoch 5\n",
            "train loss: 0.0009322680261731148, train accuracy: 0.9534, train f1 score: 0.9533999188040185 (positive: 0.9534614309111973, negative: 0.9533384066968398)\n",
            "test loss: 0.002690666387081146, test accuracy: 0.8752, test f1 score: 0.8751488537790819 (positive: 0.8776758409785933, negative: 0.8726218665795704)\n",
            "================================================================================================================================================================\n",
            "Epoch 6\n",
            "train loss: 0.0007794476750493049, train accuracy: 0.96228, train f1 score: 0.9622799260686552 (positive: 0.962332734172159, negative: 0.9622271179651513)\n",
            "test loss: 0.00289961887717247, test accuracy: 0.87392, test f1 score: 0.8738288369944405 (positive: 0.870437356132851, negative: 0.87722031785603)\n",
            "================================================================================================================================================================\n",
            "Epoch 7\n",
            "train loss: 0.0006194388473033905, train accuracy: 0.96956, train f1 score: 0.9695599917690217 (positive: 0.9695758205733019, negative: 0.9695441629647416)\n",
            "test loss: 0.003018428916931152, test accuracy: 0.8744, test f1 score: 0.8743900951063155 (positive: 0.8732746791508597, negative: 0.8755055110617714)\n",
            "================================================================================================================================================================\n",
            "Epoch 8\n",
            "train loss: 0.00055157943546772, train accuracy: 0.97416, train f1 score: 0.9741599270689782 (positive: 0.9742033383915023, negative: 0.9741165157464541)\n",
            "test loss: 0.0031211083543300627, test accuracy: 0.87624, test f1 score: 0.8762385354563332 (positive: 0.8766642748943635, negative: 0.875812796018303)\n",
            "================================================================================================================================================================\n",
            "Epoch 9\n",
            "train loss: 0.00047843360375612973, train accuracy: 0.97668, train f1 score: 0.976679991604797 (positive: 0.9766939836098341, negative: 0.9766659995997599)\n",
            "test loss: 0.0032007467687129973, test accuracy: 0.87764, test f1 score: 0.8776368422474058 (positive: 0.8770152374060226, negative: 0.878258447088789)\n",
            "================================================================================================================================================================\n",
            "Epoch 10\n",
            "train loss: 0.0004378277746587992, train accuracy: 0.97888, train f1 score: 0.9788799459326616 (positive: 0.9789137380191694, negative: 0.9788461538461539)\n",
            "test loss: 0.0032887317997217177, test accuracy: 0.87636, test f1 score: 0.8763513582458873 (positive: 0.8773850608909516, negative: 0.8753176556008229)\n",
            "================================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmtntYduYPwV"
      },
      "source": [
        "Use the stored SentimentCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrmUNt1sYCxx",
        "outputId": "579a8a6c-fe56-49d1-ba80-57c6bdd5ad4d"
      },
      "source": [
        "cnn_sentiment_classifier.load_state_dict(torch.load(\"SentimentCNN.pth\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-1BBvefYTt9",
        "outputId": "4de359b6-369d-46e3-e6b6-5a197fc1acbe"
      },
      "source": [
        "cnn_sentiment_classifier.eval()\n",
        "total_loss = 0.0\n",
        "true_positive = 0\n",
        "false_negative = 0\n",
        "positive_predictions = 0\n",
        "true_negative = 0\n",
        "false_positive = 0\n",
        "negative_predictions = 0\n",
        "for inputs, labels in test_loader:\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  outputs = cnn_sentiment_classifier(inputs.swapaxes(1, 2))\n",
        "  loss = criterion(outputs, labels)\n",
        "  total_loss += loss.item()\n",
        "  _, predicted = outputs.max(1)\n",
        "\n",
        "  for prediction_index, prediction in enumerate(predicted):\n",
        "    label = labels[prediction_index].item()\n",
        "    if label == 1 and prediction == label:\n",
        "      true_positive += 1\n",
        "    if prediction == 0 and label == 1:\n",
        "      false_negative += 1\n",
        "    if prediction == 1:\n",
        "      positive_predictions += 1\n",
        "    if label == 0 and prediction == label:\n",
        "      true_negative += 1\n",
        "    if prediction == 1 and label == 0:\n",
        "      false_positive += 1\n",
        "    if prediction == 0:\n",
        "      negative_predictions += 1\n",
        "\n",
        "positive_precision = true_positive / positive_predictions if positive_predictions != 0 else 0\n",
        "positive_recall = true_positive / (true_positive + false_negative)\n",
        "positive_f1_score = 2 * (positive_precision * positive_recall) / (positive_precision + positive_recall) if positive_precision + positive_recall != 0 else 0\n",
        "negative_precision = true_negative / negative_predictions if negative_predictions != 0 else 0\n",
        "negative_recall = true_negative / (true_negative + false_positive)\n",
        "negative_f1_score = 2 * (negative_precision * negative_recall) / (negative_precision + negative_recall) if negative_precision + negative_recall != 0 else 0\n",
        "overall_f1_score = (positive_f1_score + negative_f1_score)/2\n",
        "print(f\"test loss: {total_loss/len(test_loader.dataset)}, test accuracy: {(true_positive + true_negative)/len(test_loader.dataset)}, test f1 score: {overall_f1_score} (positive: {positive_f1_score}, negative: {negative_f1_score})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss: 0.0023254449409246445, test accuracy: 0.87888, test f1 score: 0.8788782876296792 (positive: 0.8793337052681917, negative: 0.8784228699911668)\n"
          ]
        }
      ]
    }
  ]
}